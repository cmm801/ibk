{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import pytz\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_market_calendars as pmc\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname('.'), '..', 'lib'))\n",
    "sys.path.insert(0, '/Users/chris/programming/interactivebrokers')\n",
    "import ibapi\n",
    "\n",
    "sys.path.insert(0, '/Users/chris/programming/analyses/trading')\n",
    "import tradingutils as tutils\n",
    "\n",
    "MAX_WAIT_TIME = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect with unused clientId...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR -1 2104 Market data farm connection is OK:usfuture.nj\n",
      "ERROR -1 2104 Market data farm connection is OK:usfarm.nj\n",
      "ERROR -1 2104 Market data farm connection is OK:usfuture\n",
      "ERROR -1 2104 Market data farm connection is OK:cashfarm\n",
      "ERROR -1 2104 Market data farm connection is OK:usfarm\n",
      "ERROR -1 2106 HMDS data farm connection is OK:euhmds\n",
      "ERROR -1 2106 HMDS data farm connection is OK:fundfarm\n",
      "ERROR -1 2106 HMDS data farm connection is OK:ushmds\n",
      "ERROR -1 2158 Sec-def data farm connection is OK:secdefnj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "serverVersion:%s connectionTime:%s:clientId:%s\n",
      "MarketDataApp connecting to IB...\n",
      "Attempting to connect with unused clientId...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR -1 2104 Market data farm connection is OK:usfuture.nj\n",
      "ERROR -1 2104 Market data farm connection is OK:usfarm.nj\n",
      "ERROR -1 2104 Market data farm connection is OK:usfuture\n",
      "ERROR -1 2104 Market data farm connection is OK:cashfarm\n",
      "ERROR -1 2104 Market data farm connection is OK:usfarm\n",
      "ERROR -1 2106 HMDS data farm connection is OK:euhmds\n",
      "ERROR -1 2106 HMDS data farm connection is OK:fundfarm\n",
      "ERROR -1 2106 HMDS data farm connection is OK:ushmds\n",
      "ERROR -1 2158 Sec-def data farm connection is OK:secdefnj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "ERROR: request -1. (Unimplemented error handler).\n",
      "serverVersion:%s connectionTime:%s:clientId:%s\n",
      "MarketDataApp connecting to IB...\n"
     ]
    }
   ],
   "source": [
    "import master\n",
    "import marketdata\n",
    "import contracts\n",
    "import helper\n",
    "\n",
    "PORT = 7497\n",
    "md_app = marketdata.get_instance(port=PORT)\n",
    "ct_app = contracts.get_instance(port=PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_save_daily_data(md_app, contract, start_date, end_date, data_type):\n",
    "    period_start = start_date\n",
    "    while period_start < end_date:\n",
    "        assert isinstance(period_start, datetime.date), 'Start/end dates must be dates - not datetimes.'\n",
    "        \n",
    "        # Create a data request object\n",
    "        reqObj = create_daily_data_request(contract, frequency=frequency, data_type=data_type, date=period_start)\n",
    "        \n",
    "        # Download the data, if the market was open on this day\n",
    "        if reqObj is not None:\n",
    "            df = download_daily_data(reqObj)\n",
    "\n",
    "            # Save to file    \n",
    "            tws_date = datetime.datetime.strftime(period_start, '%Y%m%d')\n",
    "            filename = tutils.save_daily_ib_data(df, ticker, frequency=reqObj.frequency, \n",
    "                                                 tws_date=tws_date, data_type=data_type)\n",
    "            print('Saved data to {}'.format(filename))\n",
    "\n",
    "        # Move to the next start date\n",
    "        period_start += datetime.timedelta(days=1)\n",
    "    \n",
    "def create_daily_data_request(contract, frequency, data_type, date):\n",
    "    \"\"\"Create a data request object to get all daily data at a certain frequency.\n",
    "    \"\"\"\n",
    "    datetime_intervals, _ = get_trading_intervals(date)\n",
    "    if not datetime_intervals:\n",
    "        # Skip days with no trading hours\n",
    "        return None\n",
    "    else:\n",
    "        # Only request data between start/end of trading hours\n",
    "        start_time = datetime_intervals[0][0]\n",
    "        end_time = datetime_intervals[-1][1]\n",
    "\n",
    "        # Get the request object\n",
    "        tws_start = helper.convert_datetime_to_tws_date(start_time)\n",
    "        tws_end = helper.convert_datetime_to_tws_date(end_time)    \n",
    "        reqObjList = md_app.create_historical_data_request([contract], is_snapshot=True, frequency=frequency, \n",
    "                                                use_rth=False, data_type=data_type,\n",
    "                                                start=tws_start, end=tws_end, duration=\"\")\n",
    "        reqObj = reqObjList[0]\n",
    "        assert reqObj is not None, 'Request parameters are invalid.'\n",
    "        return reqObj\n",
    "    \n",
    "def download_daily_data(reqObj):\n",
    "    reqObj.place_request()    \n",
    "    return retrieve_requested_data(reqObj)\n",
    "\n",
    "def _get_number_of_expected_results(reqObj):\n",
    "    \"\"\" Get the number of expected results from each subrequest.\n",
    "    \"\"\"\n",
    "    sub_requests = reqObj.get_subrequests()\n",
    "    n_requests = len(sub_requests)\n",
    "    n_expected_results = [None] * n_requests\n",
    "\n",
    "    freq_helper = helper.TimeHelper(reqObj.frequency, 'frequency')\n",
    "    total_seconds_per_bar = freq_helper.total_seconds()\n",
    "    for j, sub_req in enumerate(sub_requests):\n",
    "        total_seconds_in_request = (sub_req.get_end_tws() - sub_req.get_start_tws()).total_seconds()\n",
    "        n_expected_results[j] = int(total_seconds_in_request / total_seconds_per_bar)\n",
    "    assert all([x > 0 for x in n_expected_results]), \\\n",
    "                            'All requests should expect a positive number of results.'\n",
    "    return n_expected_results\n",
    "\n",
    "def retrieve_requested_data(reqObj):\n",
    "    # Wait until the data has been returned by IB\n",
    "    data = reqObj.get_data()\n",
    "    n_requests = len(reqObj.get_subrequests())\n",
    "    n_expected_results = _get_number_of_expected_results(reqObj)\n",
    "    \n",
    "    def calc_count(d): \n",
    "        ct = [None] * n_requests\n",
    "        if d:\n",
    "            for idx in range(n_requests):\n",
    "                ct[idx] = len(d[idx]) == n_expected_results[idx]\n",
    "        return ct\n",
    "        \n",
    "    # Wait until all datasets have been returned\n",
    "    t0 = time.time()\n",
    "    ct = calc_count(data)\n",
    "    while sum(ct) != n_requests and time.time() - t0 < MAX_WAIT_TIME:\n",
    "        print('Waiting for results {}: {}/{}'.format(int(time.time() - t0), sum(ct), n_requests))\n",
    "        time.sleep(5)\n",
    "        ct = calc_count(data)\n",
    "\n",
    "    if sum(ct) != n_requests:\n",
    "        # If some requests were not successful, we need to re-run them\n",
    "        print(\"Restarting unsuccessful request(s)...\")\n",
    "        sub_requests = reqObj.get_subrequests()\n",
    "        for j in range(n_requests):\n",
    "            if len(data[j]) < n_expected_results[j]:\n",
    "                sub_requests[j].reset()\n",
    "                sub_requests[j].place_request()\n",
    "                \n",
    "        return retrieve_requested_data(reqObj)\n",
    "    else:\n",
    "        print('Finished downloading for {}. Time elapsed: {}'.format(reqObj.start, time.time() - t0))\n",
    "        return reqObj.get_dataframe()\n",
    "\n",
    "def download_all_requests_in_list(reqObjList):\n",
    "    for j, reqObj in enumerate(reqObjList):\n",
    "        print('Processing request {} for {} {} from {} to {}...'.format(j, reqObj.contract.localSymbol, \n",
    "                                                reqObj.data_type, reqObj.start, reqObj.end))\n",
    "\n",
    "        success = False\n",
    "        t0 = time.time()\n",
    "        while not success:\n",
    "            # Place the time series request\n",
    "            reqObj.place_request()\n",
    "\n",
    "            # Wait until the data has been returned by IB\n",
    "            data = retrieve_requested_data(reqObj)\n",
    "\n",
    "        print('Finished. Time elapsed: {}'.format(time.time() - t0))\n",
    "\n",
    "        # Extract and format data\n",
    "        df = reqObj.get_dataframe()\n",
    "\n",
    "        # Save to file\n",
    "        tws_date = reqObj.start.split(' ')[0]\n",
    "        symbol = reqObj.contract.localSymbol\n",
    "        filename = tutils.save_daily_ib_data(df, ticker, frequency=reqObj.frequency, \n",
    "                                             tws_date=tws_date, data_type=reqObj.data_type)\n",
    "        print(filename)\n",
    "        \n",
    "def get_trading_intervals(date):    \n",
    "    # Create a datetime representing midnight on the target date\n",
    "    dt = datetime.datetime.combine(date, datetime.datetime.min.time())\n",
    "    \n",
    "    if dt.weekday() < 4:\n",
    "        # Monday to Thursday trading is open all day except\n",
    "        #     closing from 15:15 - 15:30 CT and 16:00 - 17:00 CT for scheduled maintenance\n",
    "        trading_intervals = [(dt, dt + datetime.timedelta(minutes=16*60 + 15)),\n",
    "                             (dt + datetime.timedelta(minutes=16*60 + 30), dt + datetime.timedelta(hours=17)),\n",
    "                             (dt + datetime.timedelta(hours=18), dt + datetime.timedelta(hours=24))]\n",
    "    elif dt.weekday() == 4:\n",
    "        # Friday trading is open until 16:00 CT\n",
    "        trading_intervals = [(dt, dt + datetime.timedelta(minutes=16*60 + 15)),\n",
    "                             (dt + datetime.timedelta(minutes=16*60 + 30), dt + datetime.timedelta(hours=17))]\n",
    "        \n",
    "    elif dt.weekday() == 5:\n",
    "        # Market is closed on Saturdays    \n",
    "        trading_intervals = []\n",
    "    elif dt.weekday() == 6:\n",
    "        # Sunday trading opens at 5 pm CT/6 pm EST\n",
    "        trading_intervals = [(dt + datetime.timedelta(hours=18), dt + datetime.timedelta(hours=24))]\n",
    "\n",
    "    # Specify that all of these dates are in EST time zone\n",
    "    tzone = pytz.timezone(helper.TIMEZONE_EST)\n",
    "    datetime_intervals = [None] * len(trading_intervals)\n",
    "    timestamp_intervals = [None] * len(trading_intervals)\n",
    "    for j in range(len(trading_intervals)):\n",
    "        start = tzone.localize(trading_intervals[j][0])\n",
    "        end = tzone.localize(trading_intervals[j][1])\n",
    "        datetime_intervals[j] = (start, end) \n",
    "        timestamp_intervals[j] = (helper.get_utc_timestamp_from_datetime(start),\n",
    "                                 helper.get_utc_timestamp_from_datetime(end))   \n",
    "\n",
    "    return datetime_intervals, timestamp_intervals\n",
    "\n",
    "def is_data_within_trading_hours(df, date):\n",
    "    _, timestamp_intervals = get_trading_intervals(date)\n",
    "    \n",
    "    if not timestamp_intervals:\n",
    "        return df.shape[0] == 0\n",
    "    else:\n",
    "        if df.index.values[0] < timestamp_intervals[0][0]:\n",
    "            return False\n",
    "        if df.index.values[-1] > timestamp_intervals[-1][1]:\n",
    "            return False        \n",
    "\n",
    "        # Check that all data points are within a valid interval\n",
    "        valid = np.zeros_like(df.index.values, dtype=bool)\n",
    "        for interval in timestamp_intervals:\n",
    "            start, end = interval\n",
    "            is_in_interval = (start <= df.index.values) & (df.index.values <= end)\n",
    "            valid = valid | is_in_interval\n",
    "            \n",
    "        return np.all(valid)      \n",
    "    \n",
    "def find_gaps_in_data(df, date, min_gap=300):\n",
    "    utc_datetimes = [tutils.convert_tws_date_to_utc_datetime(d) for d in df.date.values]\n",
    "    utc_timestamps = [dt.timestamp() for dt in utc_datetimes]\n",
    "    df.index = pd.Index(utc_timestamps, name='utc_timestamp')\n",
    "\n",
    "    _, timestamp_intervals = get_trading_intervals(date)\n",
    "\n",
    "    if not timestamp_intervals:\n",
    "        print('{} should be empty'.format(date))\n",
    "        return []\n",
    "    elif not is_data_within_trading_hours(df, date):\n",
    "        print('{}: Some data occurs outside of trading hours.'.format(date))\n",
    "        return []\n",
    "    else:\n",
    "        # Add times outside of trading hours to the index values so that we can \n",
    "        #    more easily gauge where there is missing data\n",
    "        index_vals = np.sort(df.index.values)\n",
    "        start_day = timestamp_intervals[0][0]\n",
    "        end_day = timestamp_intervals[-1][1]\n",
    "        if index_vals[0] != start_day:\n",
    "            index_vals = np.hstack([start_day, index_vals])\n",
    "        if index_vals[-1] != end_day:\n",
    "            index_vals = np.hstack([index_vals, end_day])\n",
    "        for j in range(1, len(timestamp_intervals)):\n",
    "            close_time = timestamp_intervals[j-1][1]\n",
    "            open_time = timestamp_intervals[j][0]\n",
    "            nontrading_times = np.arange(close_time, open_time)\n",
    "            index_vals = np.sort(np.hstack([index_vals, nontrading_times]))\n",
    "\n",
    "        diff = index_vals[1:] - index_vals[:-1]\n",
    "        gaps = []\n",
    "        for idx in np.where(diff > min_gap)[0]:\n",
    "            gaps.append((diff[idx],\n",
    "                         idx+1, \n",
    "                         helper.convert_utc_timestamp_to_datetime(index_vals[idx], tz_name=helper.TIMEZONE_EST),\n",
    "                         helper.convert_utc_timestamp_to_datetime(index_vals[idx+1], tz_name=helper.TIMEZONE_EST),\n",
    "                        ))    \n",
    "        return gaps    \n",
    "    \n",
    "def get_exchange_trading_hours(exchange, start_date, end_date):\n",
    "    calendar = pmc.get_calendar(exchange)\n",
    "    utc_trading_hours = calendar.schedule(start_date=start_date, end_date=end_date)\n",
    "\n",
    "    def convert_utc_datetime_to_est(utc_hours):\n",
    "        datetimes = pd.DatetimeIndex(utc_hours.values)\n",
    "        tz_est = pytz.timezone(helper.TIMEZONE_EST)\n",
    "        est_hours = []\n",
    "        for utc_dt in datetimes:\n",
    "            est_hours.append(pytz.utc.localize(utc_dt).astimezone(tz_est))\n",
    "        return est_hours\n",
    "\n",
    "    est_trading_hours = utc_trading_hours.copy()\n",
    "    open_hours = convert_utc_datetime_to_est(utc_trading_hours.market_open)\n",
    "    est_trading_hours.market_open = [d - datetime.timedelta(minutes=1) for d in open_hours]\n",
    "    est_trading_hours.market_close = convert_utc_datetime_to_est(utc_trading_hours.market_close)\n",
    "    return est_trading_hours    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_contract = ibapi.contract.Contract()\n",
    "partial_contract.symbol = 'VIX'\n",
    "partial_contract.secType = 'FUT'\n",
    "partial_contract.lastTradeDateOrContractMonth = '202005'\n",
    "partial_contract.currency = 'USD'\n",
    "\n",
    "#ct_app.match_contract(partial_contract)\n",
    "matching_contracts = ct_app.get_all_matching_contracts(partial_contract)\n",
    "matching_contracts\n",
    "\n",
    "#contract = matching_contracts[0]\n",
    "#ct_app.add_to_saved_contracts([contract])\n",
    "#ct_app.get_contract(contract.localSymbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Waiting for results 0: 46/48\n",
      "Finished downloading for 20200324 00:00:00. Time elapsed: 5.000666856765747\n",
      "Saved data to data/ESM0_1s_20200324_TRADES.csv\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping 556.9834660053253 seconds to avoid pacing violation on total requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR -1 2104 Market data farm connection is OK:usfuture.nj\n",
      "ERROR -1 2104 Market data farm connection is OK:usfuture.nj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: request -1\n",
      "ERROR: request -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usfuture.nj\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usfuture.nj\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usfuture.nj\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usfuture.nj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: request -1\n",
      "ERROR: request -1\n",
      "ERROR: request -1\n",
      "ERROR: request -1\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Waiting for results 0: 47/48\n",
      "Finished downloading for 20200325 00:00:00. Time elapsed: 5.005151987075806\n",
      "Saved data to data/ESM0_1s_20200325_TRADES.csv\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping 555.3613220214844 seconds to avoid pacing violation on total requests.\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Waiting for results 0: 46/48\n",
      "Finished downloading for 20200326 00:00:00. Time elapsed: 5.000184059143066\n",
      "Saved data to data/ESM0_1s_20200326_TRADES.csv\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping 1.7017317771911622 seconds to avoid pacing violation on total requests.\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Waiting for results 0: 32/34\n",
      "Finished downloading for 20200327 00:00:00. Time elapsed: 5.000141143798828\n",
      "Saved data to data/ESM0_1s_20200327_TRADES.csv\n"
     ]
    }
   ],
   "source": [
    "ticker = 'ESM0'\n",
    "frequency = '1s'\n",
    "data_types = ['TRADES']\n",
    "\n",
    "start = datetime.date(2020, 3, 24)\n",
    "end = datetime.date(2020, 3, 28)\n",
    "\n",
    "contract = ct_app.get_contract(ticker)\n",
    "for data_type in data_types:\n",
    "    download_and_save_daily_data(md_app, contract, start, end, data_type=data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series request\n",
    "ticker = 'ESM0'\n",
    "frequency = '1s'\n",
    "data_type = 'ASK'\n",
    "start = '20200315 00:00:00'\n",
    "end = '20200316 00:00:00'\n",
    "duration = ''\n",
    "\n",
    "contractList = [ct_app.get_contract(ticker)]\n",
    "reqObjList = md_app.create_historical_data_request(contractList, is_snapshot=True, frequency=frequency, \n",
    "                                        use_rth=False, data_type=data_type,\n",
    "                                        start=start, end=end, duration=duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n",
      "Sleeping to avoid pacing violation from requests on same contract...\n"
     ]
    }
   ],
   "source": [
    "# Place the time series request\n",
    "reqObj = reqObjList[0]\n",
    "reqObj.place_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished downloading for 20200315 00:00:00. Time elapsed: 2.002716064453125e-05\n"
     ]
    }
   ],
   "source": [
    "df = retrieve_requested_data(reqObj)\n",
    "tutils.set_timestamp_index(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to data/ESM0_1M_20200315_TRADES.csv\n"
     ]
    }
   ],
   "source": [
    "tws_date = start.split(' ')[0]\n",
    "filename = tutils.save_daily_ib_data(df, reqObj.contract.localSymbol, frequency=reqObj.frequency, \n",
    "                                     tws_date=tws_date, data_type=reqObj.data_type)\n",
    "print('Saved to {}'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'ESM0'\n",
    "start = datetime.datetime(2020, 3, 15)\n",
    "end = datetime.datetime(2020, 4, 20)\n",
    "data_type = 'ASK'\n",
    "\n",
    "frequency = '1s'\n",
    "files = tutils._get_valid_files(ticker, frequency, start, end, data_type)\n",
    "data_frames_1s = dict()\n",
    "for f in files:\n",
    "    date = f.split('_')[-2]\n",
    "    df = pd.read_csv(f)\n",
    "    df.set_index('utc_timestamp', inplace=True)\n",
    "    data_frames_1s[date] = df\n",
    "    \n",
    "frequency = '1M'\n",
    "files = tutils._get_valid_files(ticker, frequency, start, end, data_type)\n",
    "data_frames_1M = dict()\n",
    "for f in files:\n",
    "    date = f.split('_')[-2]\n",
    "    df = pd.read_csv(f)\n",
    "    df.set_index('utc_timestamp', inplace=True)\n",
    "    data_frames_1M[date] = df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = start\n",
    "status = dict()\n",
    "t0 = time.time()\n",
    "while date <= min(end, datetime.datetime.now()):\n",
    "    tws_date = datetime.datetime.strftime(date, '%Y%m%d')\n",
    "    \n",
    "    datetime_intervals, timestamp_intervals = get_trading_intervals(date)\n",
    "    if not timestamp_intervals:\n",
    "        pass\n",
    "    elif tws_date not in data_frames_1s:\n",
    "        status[date] = ['Missing seconds']\n",
    "    elif tws_date not in data_frames_1M:\n",
    "        status[date] = ['Missing minutes']\n",
    "    else:\n",
    "        df_S = data_frames_1s[tws_date]\n",
    "        df_M = data_frames_1M[tws_date]\n",
    "\n",
    "        index_vals_S = np.floor(df_S.index.values / 60) * 60\n",
    "        index_vals_S = set(index_vals_S.astype(int))\n",
    "\n",
    "        if data_type == 'TRADES':\n",
    "            index_vals_M = set(df_M.iloc[df_M.volume.values > 0].index.astype(int).values)\n",
    "        else:\n",
    "            index_vals_M = set(df_M.index.astype(int).values)\n",
    "\n",
    "        tz_est = pytz.timezone(helper.TIMEZONE_EST)\n",
    "        status[tws_date] = [helper.convert_utc_timestamp_to_datetime(d).astimezone(tz_est) for d \\\n",
    "                                             in sorted(index_vals_M - index_vals_S)]        \n",
    "    # Move to the next date\n",
    "    date = date + datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20200315': [],\n",
       " '20200316': [],\n",
       " '20200317': [],\n",
       " '20200318': [],\n",
       " '20200319': [],\n",
       " '20200320': [],\n",
       " '20200322': [],\n",
       " '20200323': [],\n",
       " '20200324': [],\n",
       " '20200325': [],\n",
       " '20200326': [],\n",
       " '20200327': [],\n",
       " '20200329': [],\n",
       " '20200330': [],\n",
       " '20200331': [],\n",
       " '20200401': [],\n",
       " '20200402': [],\n",
       " '20200403': [],\n",
       " '20200405': [],\n",
       " '20200406': [],\n",
       " '20200407': [],\n",
       " '20200408': [],\n",
       " '20200409': [],\n",
       " datetime.datetime(2020, 4, 10, 0, 0): ['Missing seconds'],\n",
       " '20200412': [],\n",
       " '20200413': [],\n",
       " '20200414': [],\n",
       " '20200415': [],\n",
       " '20200416': [],\n",
       " '20200417': [],\n",
       " '20200419': [],\n",
       " datetime.datetime(2020, 4, 20, 0, 0): ['Missing seconds']}"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR -1 2104 Market data farm connection is OK:usfuture.nj\n",
      "ERROR -1 2104 Market data farm connection is OK:usfuture.nj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: request -1\n",
      "ERROR: request -1\n"
     ]
    }
   ],
   "source": [
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200315: Shapes disagree: (15, 5) vs. (16, 5)\n",
      "20200316: Shapes disagree: (774, 5) vs. (777, 5)\n",
      "20200317: Shapes disagree: (1279, 5) vs. (1287, 5)\n",
      "20200318: Shapes disagree: (867, 5) vs. (872, 5)\n",
      "20200320: Disagree on column high\n",
      "20200322: Shapes disagree: (264, 5) vs. (266, 5)\n",
      "20200324: Shapes disagree: (1257, 5) vs. (1264, 5)\n",
      "20200326: Disagree on column high\n",
      "20200326: Disagree on column low\n",
      "20200327: Disagree on column low\n",
      "20200330: Shapes disagree: (1365, 5) vs. (1364, 5)\n",
      "20200331: Shapes disagree: (1365, 5) vs. (1364, 5)\n",
      "20200401: Disagree on column low\n",
      "20200402: Shapes disagree: (1365, 5) vs. (1362, 5)\n",
      "20200403: Disagree on column low\n",
      "20200405: Disagree on column low\n",
      "20200406: Disagree on column high\n",
      "20200406: Disagree on column low\n",
      "20200407: Shapes disagree: (1365, 5) vs. (1364, 5)\n",
      "20200408: Shapes disagree: (1365, 5) vs. (1364, 5)\n",
      "20200409: Disagree on column high\n",
      "20200409: Disagree on column low\n",
      "20200412: Disagree on column high\n",
      "20200413: Shapes disagree: (1365, 5) vs. (1362, 5)\n",
      "20200414: Disagree on index values\n",
      "20200414: Disagree on column close\n",
      "20200414: Disagree on column date\n",
      "20200414: Disagree on column high\n",
      "20200414: Disagree on column low\n",
      "20200414: Disagree on column open\n",
      "20200415: Shapes disagree: (1365, 5) vs. (1364, 5)\n",
      "20200416: Shapes disagree: (1365, 5) vs. (1364, 5)\n",
      "20200417: Shapes disagree: (1005, 5) vs. (1004, 5)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "for date, df_M in data_frames_1M.items():\n",
    "    if 'volume' in df_M.columns:\n",
    "        df_M = df_M.iloc[df_M.volume.values > 0]\n",
    "    df_M['date'] = [re.sub(' +', ' ', d) for d in df_M.date.values]\n",
    "    \n",
    "    input_df_S = data_frames_1s[date]\n",
    "    df_S = tutils.downsample(input_df_S, '1M')\n",
    "    if 'volume' in df_S.columns:\n",
    "        df_S = df_S.iloc[df_S.volume.values > 0]\n",
    "    \n",
    "    if df_S.shape != df_M.shape:\n",
    "        print('{}: Shapes disagree: {} vs. {}'.format(date, df_S.shape, df_M.shape))\n",
    "    else:\n",
    "        if not np.all(df_S.index.values == df_M.index.values):\n",
    "            print('{}: Disagree on index values'.format(date))        \n",
    "        for col in df_S.columns:\n",
    "            if col == 'date':\n",
    "                if not np.all(df_S.date.values == df_M.date.values):\n",
    "                    print('{}: Disagree on column {}'.format(date, col))\n",
    "            else:\n",
    "                if not np.all(np.isclose(df_S[col].values, df_M[col].values)):\n",
    "                    print('{}: Disagree on column {}'.format(date, col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '20200320'\n",
    "input_df = data_frames_1s[date]\n",
    "df_M = data_frames_1M[date]\n",
    "\n",
    "if 'volume' in df_M.columns:\n",
    "    df_M = df_M.iloc[df_M.volume.values > 0]\n",
    "df_M['date'] = [re.sub(' +', ' ', d) for d in df_M.date.values]\n",
    "\n",
    "input_df_S = data_frames_1s[date]\n",
    "df_S = tutils.downsample(input_df_S, '1M')\n",
    "if 'volume' in df_S.columns:\n",
    "    df_S = df_S.iloc[df_S.volume.values > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[close              2435.75\n",
       " date     20200320 03:15:00\n",
       " high               2436.25\n",
       " low                2433.25\n",
       " open               2436.25\n",
       " Name: 1584688500.0, dtype: object, close              2435.75\n",
       " date     20200320 03:15:00\n",
       " high                2436.5\n",
       " low                2433.25\n",
       " open               2436.25\n",
       " Name: 1584688500.0, dtype: object]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usopt\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usopt\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usopt\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usopt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: request -1\n",
      "ERROR: request -1\n",
      "ERROR: request -1\n",
      "ERROR: request -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR -1 2104 Market data farm connection is OK:usopt\n",
      "ERROR -1 2104 Market data farm connection is OK:usopt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: request -1\n",
      "ERROR: request -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR -1 2104 Market data farm connection is OK:usfuture.nj\n",
      "ERROR -1 2104 Market data farm connection is OK:usfuture.nj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: request -1\n",
      "ERROR: request -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usopt\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usopt\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usopt\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usopt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: request -1\n",
      "ERROR: request -1\n",
      "ERROR: request -1\n",
      "ERROR: request -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR -1 2104 Market data farm connection is OK:usopt\n",
      "ERROR -1 2104 Market data farm connection is OK:usopt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: request -1\n",
      "ERROR: request -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usfuture.nj\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usfuture.nj\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usfuture.nj\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usfuture.nj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: request -1\n",
      "ERROR: request -1\n",
      "ERROR: request -1\n",
      "ERROR: request -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usopt\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usopt\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usopt\n",
      "ERROR -1 2108 Market data farm connection is inactive but should be available upon demand.usopt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: request -1\n",
      "ERROR: request -1\n",
      "ERROR: request -1\n",
      "ERROR: request -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR -1 2104 Market data farm connection is OK:usopt\n",
      "ERROR -1 2104 Market data farm connection is OK:usopt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: request -1\n",
      "ERROR: request -1\n"
     ]
    }
   ],
   "source": [
    "np.where(df_S.high.values != df_M.high.values)\n",
    "[df_S.iloc[195], df_M.iloc[195]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all of the time series request objects at the same time\n",
    "tickers = [ 'ESM0']\n",
    "frequency = '1s'\n",
    "data_types = ['TRADES', 'BID', 'ASK']\n",
    "start_dates = ['20200417 00:00:00']\n",
    "end_dates = ['20200418 00:00:00']\n",
    "duration = ''\n",
    "\n",
    "reqObjList = []\n",
    "for ticker in tickers:\n",
    "    contractList = [ct_app.get_contract(ticker)]\n",
    "    for data_type in data_types:\n",
    "        for j in range(len(start_dates)):\n",
    "            _single_req_obj = md_app.create_historical_data_request(contractList, is_snapshot=True, \n",
    "                                            frequency=frequency, use_rth=False, data_type=data_type,\n",
    "                                            start=start_dates[j], end=end_dates[j], duration=duration)\n",
    "            reqObjList.append(_single_req_obj[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_gap = 300\n",
    "intraday_gaps = []\n",
    "for tws_date, df in data_frames.items():\n",
    "    date = helper.convert_tws_date_to_datetime(tws_date)\n",
    "    gaps = find_gaps_in_data(df, date)\n",
    "    intraday_gaps.append(gaps, min_gap=min_gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20200315', 6, 62, 0]\n",
      "['20200316', 0, 79, 1]\n",
      "['20200317', 1, 14, 2]\n",
      "['20200318', 2, 97, 3]\n",
      "['20200322', 6, 9, 6]\n",
      "['20200324', 1, 15, 8]\n",
      "['20200330', 0, 1, 13]\n",
      "['20200409', 3, 1, 22]\n"
     ]
    }
   ],
   "source": [
    "for j, d in enumerate(data_frames.keys()):\n",
    "    date = helper.convert_tws_date_to_datetime(d)\n",
    "    ct = [len(x) for x in intraday_gaps]\n",
    "    if ct[j] > 0:\n",
    "        print([d, date.weekday(), ct[j], j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(171.0,\n",
       "  999,\n",
       "  datetime.datetime(2020, 3, 15, 18, 40, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 18, 42, 51, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (159.0,\n",
       "  1025,\n",
       "  datetime.datetime(2020, 3, 15, 18, 54, 44, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 18, 57, 23, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (146.0,\n",
       "  1040,\n",
       "  datetime.datetime(2020, 3, 15, 19, 2, 17, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 4, 43, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (102.0,\n",
       "  1046,\n",
       "  datetime.datetime(2020, 3, 15, 19, 7, 41, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 9, 23, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (154.0,\n",
       "  1049,\n",
       "  datetime.datetime(2020, 3, 15, 19, 9, 36, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 12, 10, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (157.0,\n",
       "  1053,\n",
       "  datetime.datetime(2020, 3, 15, 19, 14, 42, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 17, 19, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (252.0,\n",
       "  1059,\n",
       "  datetime.datetime(2020, 3, 15, 19, 22, 37, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 26, 49, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (135.0,\n",
       "  1060,\n",
       "  datetime.datetime(2020, 3, 15, 19, 26, 49, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 29, 4, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (191.0,\n",
       "  1067,\n",
       "  datetime.datetime(2020, 3, 15, 19, 31, 42, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 34, 53, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (289.0,\n",
       "  1068,\n",
       "  datetime.datetime(2020, 3, 15, 19, 34, 53, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 39, 42, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (155.0,\n",
       "  1069,\n",
       "  datetime.datetime(2020, 3, 15, 19, 39, 42, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 42, 17, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (108.0,\n",
       "  1070,\n",
       "  datetime.datetime(2020, 3, 15, 19, 42, 17, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 44, 5, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (168.0,\n",
       "  1074,\n",
       "  datetime.datetime(2020, 3, 15, 19, 46, 30, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 49, 18, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (117.0,\n",
       "  1078,\n",
       "  datetime.datetime(2020, 3, 15, 19, 49, 43, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 51, 40, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (123.0,\n",
       "  1080,\n",
       "  datetime.datetime(2020, 3, 15, 19, 53, 1, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 55, 4, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (142.0,\n",
       "  1084,\n",
       "  datetime.datetime(2020, 3, 15, 19, 56, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 19, 58, 22, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (146.0,\n",
       "  1086,\n",
       "  datetime.datetime(2020, 3, 15, 19, 58, 51, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 20, 1, 17, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (258.0,\n",
       "  1091,\n",
       "  datetime.datetime(2020, 3, 15, 20, 2, 3, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 20, 6, 21, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (313.0,\n",
       "  1092,\n",
       "  datetime.datetime(2020, 3, 15, 20, 6, 21, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 20, 11, 34, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (145.0,\n",
       "  1093,\n",
       "  datetime.datetime(2020, 3, 15, 20, 11, 34, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 20, 13, 59, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (424.0,\n",
       "  1095,\n",
       "  datetime.datetime(2020, 3, 15, 20, 14, 13, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 20, 21, 17, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (103.0,\n",
       "  1096,\n",
       "  datetime.datetime(2020, 3, 15, 20, 21, 17, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 20, 23, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (366.0,\n",
       "  1097,\n",
       "  datetime.datetime(2020, 3, 15, 20, 23, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 20, 29, 6, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (115.0,\n",
       "  1103,\n",
       "  datetime.datetime(2020, 3, 15, 20, 32, 20, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 20, 34, 15, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (133.0,\n",
       "  1107,\n",
       "  datetime.datetime(2020, 3, 15, 20, 34, 54, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 20, 37, 7, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (105.0,\n",
       "  1108,\n",
       "  datetime.datetime(2020, 3, 15, 20, 37, 7, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 20, 38, 52, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (214.0,\n",
       "  1114,\n",
       "  datetime.datetime(2020, 3, 15, 20, 41, 22, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 20, 44, 56, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (739.0,\n",
       "  1116,\n",
       "  datetime.datetime(2020, 3, 15, 20, 46, 2, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 20, 58, 21, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (147.0,\n",
       "  1118,\n",
       "  datetime.datetime(2020, 3, 15, 20, 59, 36, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 21, 2, 3, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (141.0,\n",
       "  1119,\n",
       "  datetime.datetime(2020, 3, 15, 21, 2, 3, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 21, 4, 24, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (357.0,\n",
       "  1120,\n",
       "  datetime.datetime(2020, 3, 15, 21, 4, 24, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 21, 10, 21, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (268.0,\n",
       "  1132,\n",
       "  datetime.datetime(2020, 3, 15, 21, 19, 48, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 21, 24, 16, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (236.0,\n",
       "  1133,\n",
       "  datetime.datetime(2020, 3, 15, 21, 24, 16, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 21, 28, 12, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (117.0,\n",
       "  1134,\n",
       "  datetime.datetime(2020, 3, 15, 21, 28, 12, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 21, 30, 9, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (176.0,\n",
       "  1140,\n",
       "  datetime.datetime(2020, 3, 15, 21, 35, 53, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 21, 38, 49, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (366.0,\n",
       "  1141,\n",
       "  datetime.datetime(2020, 3, 15, 21, 38, 49, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 21, 44, 55, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (101.0,\n",
       "  1142,\n",
       "  datetime.datetime(2020, 3, 15, 21, 44, 55, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 21, 46, 36, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (165.0,\n",
       "  1143,\n",
       "  datetime.datetime(2020, 3, 15, 21, 46, 36, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 21, 49, 21, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (225.0,\n",
       "  1145,\n",
       "  datetime.datetime(2020, 3, 15, 21, 50, 34, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 21, 54, 19, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (244.0,\n",
       "  1148,\n",
       "  datetime.datetime(2020, 3, 15, 21, 55, 8, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 21, 59, 12, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (131.0,\n",
       "  1151,\n",
       "  datetime.datetime(2020, 3, 15, 22, 0, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 22, 2, 11, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (236.0,\n",
       "  1154,\n",
       "  datetime.datetime(2020, 3, 15, 22, 5, 7, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 22, 9, 3, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (358.0,\n",
       "  1155,\n",
       "  datetime.datetime(2020, 3, 15, 22, 9, 3, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 22, 15, 1, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (784.0,\n",
       "  1156,\n",
       "  datetime.datetime(2020, 3, 15, 22, 15, 1, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 22, 28, 5, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (269.0,\n",
       "  1161,\n",
       "  datetime.datetime(2020, 3, 15, 22, 30, 34, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 22, 35, 3, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (116.0,\n",
       "  1165,\n",
       "  datetime.datetime(2020, 3, 15, 22, 36, 54, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 22, 38, 50, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (382.0,\n",
       "  1166,\n",
       "  datetime.datetime(2020, 3, 15, 22, 38, 50, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 22, 45, 12, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (486.0,\n",
       "  1168,\n",
       "  datetime.datetime(2020, 3, 15, 22, 46, 12, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 22, 54, 18, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (422.0,\n",
       "  1169,\n",
       "  datetime.datetime(2020, 3, 15, 22, 54, 18, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 23, 1, 20, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (128.0,\n",
       "  1171,\n",
       "  datetime.datetime(2020, 3, 15, 23, 1, 47, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 23, 3, 55, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (213.0,\n",
       "  1172,\n",
       "  datetime.datetime(2020, 3, 15, 23, 3, 55, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 23, 7, 28, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (167.0,\n",
       "  1175,\n",
       "  datetime.datetime(2020, 3, 15, 23, 9, 11, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 23, 11, 58, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (117.0,\n",
       "  1176,\n",
       "  datetime.datetime(2020, 3, 15, 23, 11, 58, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 23, 13, 55, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (490.0,\n",
       "  1177,\n",
       "  datetime.datetime(2020, 3, 15, 23, 13, 55, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 23, 22, 5, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (125.0,\n",
       "  1178,\n",
       "  datetime.datetime(2020, 3, 15, 23, 22, 5, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 23, 24, 10, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (127.0,\n",
       "  1180,\n",
       "  datetime.datetime(2020, 3, 15, 23, 24, 51, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 23, 26, 58, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (170.0,\n",
       "  1181,\n",
       "  datetime.datetime(2020, 3, 15, 23, 26, 58, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 23, 29, 48, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (146.0,\n",
       "  1182,\n",
       "  datetime.datetime(2020, 3, 15, 23, 29, 48, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 23, 32, 14, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (446.0,\n",
       "  1184,\n",
       "  datetime.datetime(2020, 3, 15, 23, 32, 27, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 23, 39, 53, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (247.0,\n",
       "  1185,\n",
       "  datetime.datetime(2020, 3, 15, 23, 39, 53, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 23, 44, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (603.0,\n",
       "  1186,\n",
       "  datetime.datetime(2020, 3, 15, 23, 44, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 15, 23, 54, 3, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>)),\n",
       " (239.0,\n",
       "  1190,\n",
       "  datetime.datetime(2020, 3, 15, 23, 56, 1, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>),\n",
       "  datetime.datetime(2020, 3, 16, 0, 0, tzinfo=<DstTzInfo 'US/Eastern' EDT-1 day, 20:00:00 DST>))]"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intraday_gaps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "intraday_gaps = []\n",
    "for tws_date, df in data_frames.items():\n",
    "    date = helper.convert_tws_date_to_datetime(tws_date)\n",
    "    \n",
    "    utc_datetimes = [tutils.convert_tws_date_to_utc_datetime(d) for d in df.date.values]\n",
    "    utc_timestamps = [dt.timestamp() for dt in utc_datetimes]\n",
    "    df.index = pd.Index(utc_timestamps, name='utc_timestamp')\n",
    "    \n",
    "    _, timestamp_intervals = get_trading_intervals(date)\n",
    "    if not timestamp_intervals:\n",
    "        print('{} should be empty'.format(tws_date))\n",
    "    else:\n",
    "        if min(df.index.values) < timestamp_intervals[0][0]:\n",
    "            print('Data precedes start for {}'.format(tws_date))\n",
    "        if max(df.index.values) > timestamp_intervals[-1][1]:\n",
    "            print('Data follows end for {}'.format(tws_date))            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ESM0_1s_20200315_BID.csv\n",
      "(2114, 846)\n",
      "data/ESM0_1s_20200322_BID.csv\n",
      "(14850, 13425)\n",
      "data/ESM0_1s_20200329_BID.csv\n",
      "(17680, 16680)\n",
      "data/ESM0_1s_20200405_BID.csv\n",
      "(16120, 15381)\n",
      "data/ESM0_1s_20200412_BID.csv\n",
      "(14836, 14259)\n"
     ]
    }
   ],
   "source": [
    "# Remove extra rows and resave the data frame\n",
    "ticker = 'ESM0'\n",
    "frequency = '1s'\n",
    "for tws_date, df in data_frames.items():\n",
    "    date = helper.convert_tws_date_to_datetime(tws_date)\n",
    "    if date.weekday() == 6:\n",
    "        utc_datetimes = [tutils.convert_tws_date_to_utc_datetime(d) for d in df.date.values]\n",
    "        utc_timestamps = [dt.timestamp() for dt in utc_datetimes]\n",
    "        df.index = pd.Index(utc_timestamps, name='utc_timestamp')\n",
    "\n",
    "        # Get the trading hours\n",
    "        _, timestamp_intervals = get_trading_intervals(date)\n",
    "\n",
    "        # Save the data within the start/end dates\n",
    "        new_df = df.iloc[ (df.index.values >= timestamp_intervals[0][0]) & \\\n",
    "                          (df.index.values <= timestamp_intervals[-1][1])]\n",
    "        tutils.set_timestamp_index(new_df)        \n",
    "        filename = tutils.save_daily_ib_data(new_df, ticker, frequency=frequency, \n",
    "                                             tws_date=tws_date, data_type=data_type)\n",
    "        print(filename)\n",
    "        print( (df.shape[0], new_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Compare tick data to 1s bars\n",
    "\n",
    "# First, create a historical time series request\n",
    "ticker = 'ESM0'\n",
    "frequency = '1s'\n",
    "data_type = 'TRADES'\n",
    "start = '20200422 08:30:00'\n",
    "end = '20200422 09:00:00'\n",
    "duration = ''\n",
    "\n",
    "contractList = [ct_app.get_contract(ticker)]\n",
    "histReqObjList = md_app.create_historical_data_request(contractList, is_snapshot=True, frequency=frequency, \n",
    "                                        use_rth=False, data_type=data_type,\n",
    "                                        start=start, end=end, duration=duration)\n",
    "\n",
    "# Place the time series request\n",
    "histReqObj = histReqObjList[0]\n",
    "histReqObj.place_request()\n",
    "\n",
    "# Get a data frame with the historical data\n",
    "df_hist = pd.DataFrame.from_dict(histReqObj.get_data()[0])\n",
    "df_hist = df_hist[df_hist['volume'] != 0]\n",
    "df_hist.reset_index(inplace=True)\n",
    "\n",
    "datetimes = pd.DatetimeIndex(df_hist.date.values).to_pydatetime()\n",
    "timestamps = [int(d.timestamp()) for d in datetimes]\n",
    "df_hist['time'] = timestamps\n",
    "df_hist.set_index('time', inplace=True)\n",
    "df_hist = df_hist.drop('date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series request\n",
    "ticker = 'ESM0'\n",
    "data_type = 'Last'\n",
    "number_of_ticks = 0\n",
    "\n",
    "contractList = [ct_app.get_contract(ticker)]\n",
    "tickReqObjList = md_app.create_streaming_tick_data_request(contractList, data_type=data_type,\n",
    "                                                    number_of_ticks=number_of_ticks)\n",
    "tickReqObj = tickReqObjList[0]\n",
    "tickReqObj.place_request()\n",
    "\n",
    "cols = ['time', 'price', 'size']\n",
    "prices = [{c: d[c] for c in cols} for d in tickReqObj.get_data()]\n",
    "df = pd.DataFrame.from_dict(prices)\n",
    "df.set_index('time', inplace=True)\n",
    "ts = tutils.convert_tick_to_time_bars(df)\n",
    "ts.index = ts.index - 6 * 3600\n",
    "\n",
    "t_0, t_F = df_hist.index.values[0], df_hist.index.values[-1]\n",
    "df_tick = ts[(t_0 <= ts.index.values) & (ts.index.values <= t_F)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the aggregated tick data to the 1s bars from TWS\n",
    "for col in df_hist.columns:\n",
    "    if not np.all(np.isclose(df_tick[col].values, df_hist[col].values)):\n",
    "        print('Values for {} do not agree.'.format(col))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python37)",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
